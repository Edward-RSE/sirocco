<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Matrix Acceleration using CUDA &mdash; python 86g documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css" />
      <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Unit Test Framework" href="tests.html" />
    <link rel="prev" title="Programming Notes" href="programmer_notes.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            python
          </a>
              <div class="version">
                86
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Documentation</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../quick.html"><em>Quick Guide to Python</em></a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../running_python.html">Running Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../input.html">Inputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../output.html">Outputs &amp; Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../plotting.html">Plotting &amp; Processing Outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operation.html">Code Operation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../radiation.html">Radiation Sources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../wind_models.html">Wind Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../coordinate.html">Coordinate grids</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../physics.html">Physics &amp; Radiative Transfer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../atomic.html">Atomic Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../meta.html">Meta-documentation</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../developer.html">Developer Documentation</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="programmer_notes.html">Programming Notes</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Matrix Acceleration using CUDA</a></li>
<li class="toctree-l2"><a class="reference internal" href="tests.html">Unit Test Framework</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../py_progs.html">Python Scripts</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">python</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content style-external-links">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../developer.html">Developer Documentation</a></li>
      <li class="breadcrumb-item active">Matrix Acceleration using CUDA</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/developer/cuda.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="matrix-acceleration-using-cuda">
<h1>Matrix Acceleration using CUDA<a class="headerlink" href="#matrix-acceleration-using-cuda" title="Permalink to this heading"></a></h1>
<p>Python can use CUDA/GPUs to accelerate solving linear systems using the cuSOLVER library, which is part of the NVIDIA
CUDA Toolkit.</p>
<p><em>This pilot study into using GPUs in Python was conducted as an HPC RSE project at the University of Southampton.</em></p>
<section id="when-should-you-use-cuda">
<h2>When should you use CUDA?<a class="headerlink" href="#when-should-you-use-cuda" title="Permalink to this heading"></a></h2>
<p>Given the pilot study nature of this work, the current matrix acceleration implementation (September 2023) is simple.
In most small to mid-sized models, using GPU acceleration will not improve model performance. As of writing,
CUDA is only used to accelerate matrix calculations specifically, so there are no performance improvements other than in
models which are bogged down by matrix calculations; e.g. such as when calculating the ionization state for a large
number of ions, or models with lots of macro atom levels/emissitivies. Even so, there may only be modest improvements in
model performance if the majority of time is spent transporting and scattering photons.</p>
<p>It is therefore only preferable to use CUDA when matrices are large enough to warrant GPU acceleration. The size of
where this is important is tricky to know, as it is hardware dependent - both on your CPU and GPU. If you are using an
old CPU, then you are likely to see improvements from matrices as small as circa 200 x 200. Otherwise, you may need to
reach up to matrix sizes of 500 x 500 (or larger!) before there is any tangible benefit.</p>
<p>This situation will be improved with smarter memory management and further parallelisation. Most time is spent copying
data back and forth between the GPU and CPU. As an example, consider the matrix ionization state calculation. Currently
only the actual step to stop the linear system (to calculate the ionization state) has been ported to the GPU. This
means each iteration toward a converged ionization state requires memory to be copied to and from the GPU, which slows
things down quite a bit. If you could instead port the entire iterative procedure to the GPU (which is not that easy),
there is no longer the need to make expensive memory copies each iteration which will significantly speed up the
algorithm.</p>
</section>
<section id="requirements">
<h2>Requirements<a class="headerlink" href="#requirements" title="Permalink to this heading"></a></h2>
<p>To use the CUDA matrix acceleration in Python, your machine needs to have the following installed,</p>
<ul class="simple">
<li><p>A CUDA-capable NVIDIA GPU</p></li>
<li><p>NVIDIA CUDA Toolkit</p></li>
<li><p>NVIDIA GPU drivers</p></li>
<li><p>A supported operating system (Windows or Linux) with a gcc compiler and toolchain</p></li>
</ul>
<p>NVIDIA provides a list of CUDA-enabled GPUs <a class="reference external" href="https://developer.nvidia.com/cuda-gpus">here</a>. Whilst the GeForce series
of NVIDIA GPUs are more affordable and generally <em>good enough</em>, from a purely raw computation standpoint NVIDIA’s
workstation and data center GPUs are more suited due differences (and additional) in hardware not included in
the GeForce line of GPUs.</p>
<section id="installing-the-cuda-toolkit">
<h3>Installing the CUDA toolkit<a class="headerlink" href="#installing-the-cuda-toolkit" title="Permalink to this heading"></a></h3>
<p>The NVIDIA CUDA Toolkit is installed either through an installer downloaded from NVIDIA or can be installed via a
package manager on Linux systems. It should be noted that the CUDA Toolkit <em>does not</em> come with NVIDIA drivers and need
to be installed separately. The NVIDIA CUDA Toolkit is available at <a class="reference external" href="https://developer.nvidia.com/cuda-downloads">https://developer.nvidia.com/cuda-downloads</a> and NVIDIA drivers at <a class="reference external" href="https://www.nvidia.co.uk/Download/index.aspx?lang=en-uk">https://www.nvidia.co.uk/Download/index.aspx</a>.</p>
<p>On Ubuntu 22.04, the toolkit and NVIDIA’s proprietary drivers are available through <code class="code docutils literal notranslate"><span class="pre">apt</span></code>,</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sudo<span class="w"> </span>apt<span class="w"> </span>install<span class="w"> </span>nvidia-cuda-toolkit<span class="w"> </span>nvidia-driver-535
</pre></div>
</div>
</section>
</section>
<section id="how-to-enable-and-run-cuda">
<h2>How to Enable and Run CUDA<a class="headerlink" href="#how-to-enable-and-run-cuda" title="Permalink to this heading"></a></h2>
<section id="compilation">
<h3>Compilation<a class="headerlink" href="#compilation" title="Permalink to this heading"></a></h3>
<p>CUDA is an additional acceleration method and is therefore not enabled by default. To enable CUDA, Python has to be
compiled with the additional <code class="code docutils literal notranslate"><span class="pre">-DCUDA_ON</span></code> flag and linked with the appropriate libraries using the NVIDIDA CUDA
compiler (nvcc). There are several ways to enable the CUDA components of Python. The most simple is to run the configure
script in the root Python directory with the arguments <code class="code docutils literal notranslate"><span class="pre">--with-cuda</span></code>,</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span><span class="nv">$PYTHON</span><span class="o">]</span><span class="w"> </span>$<span class="w"> </span>./configure<span class="w"> </span>--with-cuda

Configuring<span class="w"> </span>Makefile<span class="w"> </span><span class="k">for</span><span class="w"> </span>Python<span class="w"> </span>radiative<span class="w"> </span>transfer<span class="w"> </span>code
Checking<span class="w"> </span><span class="k">for</span><span class="w"> </span>mpicc...yes
Checking<span class="w"> </span><span class="k">for</span><span class="w"> </span>gcc...yes
Checking<span class="w"> </span><span class="k">for</span><span class="w"> </span>nvcc...yes
Preparing<span class="w"> </span>Makefile...Done.
</pre></div>
</div>
<p>If the NVIDIA CUDA Toolkit is found, you will see the output informing that the CUDA compiler <code class="code docutils literal notranslate"><span class="pre">nvcc</span></code> was found.</p>
<p>What essentially happens when you run <code class="code docutils literal notranslate"><span class="pre">code</span></code> is that a value for the variable <code class="code docutils literal notranslate"><span class="pre">NVCC</span></code> is set in the Makefile
in Python’s source directory. If you re-run <code class="code docutils literal notranslate"><span class="pre">configure</span></code> without <code class="code docutils literal notranslate"><span class="pre">--with-cuda</span></code>, then <code class="code docutils literal notranslate"><span class="pre">NVCC</span></code> will be
unset and CUDA will not be used. CUDA can be disbaled or enabled <em>“on the fly”</em> by modifying this variable without
running the configure script and by modifying the Makefile or passing the value of the variable when calling the
Makefile, e.g.,</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span><span class="nv">$PYTHON</span>/source<span class="o">]</span><span class="w"> </span>$<span class="w"> </span>make<span class="w"> </span>clean
<span class="o">[</span><span class="nv">$PYTHON</span>/source<span class="o">]</span><span class="w"> </span>$<span class="w"> </span>make<span class="w"> </span>python<span class="w"> </span><span class="nv">NVCC</span><span class="o">=</span>nvcc
</pre></div>
</div>
<p><code class="code docutils literal notranslate"><span class="pre">make</span> <span class="pre">clean</span></code> has to be run whenever CUDA is switched been enabled or disabled, due to code conditionally compiling
depending on if CUDA is enabled or not.</p>
</section>
<section id="running">
<h3>Running<a class="headerlink" href="#running" title="Permalink to this heading"></a></h3>
<p>To run Python with CUDA, you run it in the exact way even parallel models running with MPI. On a HPC system the
appropriate GPU resources will need to be requested in a job submission script. For example, on Iridis at the University
of Southampton, a functional job submission script may look like this,</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>

<span class="c1">#SBATCH --nodes=1</span>
<span class="c1">#SBATCH --ntasks-per-node=40</span>
<span class="c1">#SBATCH --time=06:00:00</span>
<span class="c1">#SBATCH --partition=gpu</span>

module<span class="w"> </span>load<span class="w"> </span>openmpi/4.1.5/gcc

mpirun<span class="w"> </span>-n<span class="w"> </span><span class="nv">$SLURM_NTASKS</span><span class="w"> </span>py<span class="w"> </span>model.pf
</pre></div>
</div>
<p>If CUDA is enabled and no GPU resources are found, Python will exit early in the program with an appropriate error
message. Note that a CUDA-aware MPI implementation is not required, as no data is communicated between GPUs.</p>
</section>
</section>
<section id="implementation">
<h2>Implementation<a class="headerlink" href="#implementation" title="Permalink to this heading"></a></h2>
<p>In this part of the documentation, we will cover the implementation details of cuSolver in Python. cuSolver is a matrix
library within the NVIDIA CUDA ecosystem, designed to accelerate both dense and sparse linear algebra problems,
including matrix factorisation, linear system solving and matrix inversion. To use cuSolver, very little GPU specific
code needs to be written, other than code to allocate memory on the GPU. There are therefore a number of similarities
between writing functions which use the cuSolver (and other CUDA mathematical libraries) and GSL libraries.</p>
<section id="the-cuda-parallel-model">
<h3>The CUDA parallel model<a class="headerlink" href="#the-cuda-parallel-model" title="Permalink to this heading"></a></h3>
<p>The main difference between CPU and GPU parallel programming is the number of (dumb) cores in a GPU. Whereas on a CPU
where we divide work on a matrix into smaller chunks, on a GPU it is realistic to have each core of the GPU operate on a
single element of the matrix whereas a CPU will likely have multiple elements. CUDA is a type of shared memory parallel
programming, and at its core are kernels, which are specialised functions designed for massive parallelism. These
kernels are executed by each thread (organized in blocks and grids), where thousands are launched and execute the code
concurrently allowing for massive parallelism.</p>
<p>As an example, consider matrix multiplication. If the calculation is parallelised, each CPU core will likely need to
calculate the matrix product for multiple elements of the matrix. On a GPU, each thread that is launched will calculate
the product for only a single element. If there are enough GPU cores available, then the calculation can be done in
effectively a single step which all threads calculating the product for each element at once.</p>
<p>A more detailed and thorough explanation of the CUDA programming model can be found in the <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#">CUDA documentation</a>.</p>
</section>
<section id="basics">
<h3>Basics<a class="headerlink" href="#basics" title="Permalink to this heading"></a></h3>
<p>Python uses the dense matrix functions in cuSolver, which are accessed through the <code class="code docutils literal notranslate"><span class="pre">cusolverDn.h</span></code> header file. To
use cuSolver, it must first be initialised. To do so, we use <code class="code docutils literal notranslate"><span class="pre">cusolverDnCreate</span></code> to create a
<code class="code docutils literal notranslate"><span class="pre">cuSolverDnHandle_t</span></code> variable which is used by cuSolver internally for resource and context management.</p>
<p>cuSolver is based on the Fortran library <a class="reference external" href="https://www.netlib.org/lapack/">LAPACK</a> and as such expects arrays to be
ordered in column-major order like in Fortran. In C, arrays are typically ordered in row-major order and so arrays must
be transposed into column-major ordering before being passed to cuSolver (an explanation of the differences between row
and column major ordering can be found <a class="reference external" href="https://en.wikipedia.org/wiki/Row-_and_column-major_order">here</a>). Matrices
can be transposed either whilst still on the CPU, or on the GPU by using a CUDA kernel as shown in the example below,</p>
<div class="literal-block-wrapper docutils container" id="id4">
<div class="code-block-caption"><span class="caption-text">A CUDA kernel to transpose a matrix from row to column major</span><a class="headerlink" href="#id4" title="Permalink to this code"></a></div>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w">  </span><span class="cm">/* __global__ is used by kernels, all of which return void */</span>
<span class="n">transpose_row_to_column_major</span><span class="p">(</span><span class="kt">double</span><span class="w"> </span><span class="o">*</span><span class="n">row_major</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="o">*</span><span class="n">column_major</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">matrix_size</span><span class="p">)</span>
<span class="p">{</span>
<span class="w">    </span><span class="cm">/* Determine the x and y coordinate for the thread -- these coords could be</span>
<span class="cm">       outside the matrix if enough threads are spawned */</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">idx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">idy</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">y</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>

<span class="w">    </span><span class="cm">/* Only transpose for threads inside the matrix */</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">idx</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">matrix_size</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">idy</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">matrix_size</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">column_major</span><span class="p">[</span><span class="n">idx</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">matrix_size</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">idy</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">row_major</span><span class="p">[</span><span class="n">idy</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">matrix_size</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">idx</span><span class="p">];</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<p>The syntax of the above is covered in detail in the <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#kernels">CUDA documentation</a>. The purpose of the kernel is take in a row
major array and to transpose it to column major.</p>
<p>Every cuSolver (and CUDA) function returns an error status. To make code more readable, a macro is usually defined which
checks the error status and raises an error message if the function does not execute successfully. This type of macro is
used extensively throughout the implementation.</p>
<div class="literal-block-wrapper docutils container" id="id5">
<div class="code-block-caption"><span class="caption-text">A useful macro for error checking cuSolver returns</span><a class="headerlink" href="#id5" title="Permalink to this code"></a></div>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="cp">#define CUSOLVER_CHECK(status)                                                                                     \</span>
<span class="cp">    do {                                                                                                           \</span>
<span class="cp">        cusolverStatus_t err = status;                                                                             \</span>
<span class="cp">        if (err != CUSOLVER_STATUS_SUCCESS) {                                                                      \</span>
<span class="cp">            Error(&quot;cuSolver Error (%d): %s (%s:%d)\n&quot;, err, cusolver_get_error_string(err), __FILE__, __LINE__);   \</span>
<span class="cp">            return err;                                                                                            \</span>
<span class="cp">        }                                                                                                          \</span>
<span class="cp">    } while (0)</span>

<span class="cm">/* Here is an example of using the macro to create a handle */</span>
<span class="n">CUSOLVER_CHECK</span><span class="p">(</span><span class="n">cusolverDnCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">handle</span><span class="p">));</span>
</pre></div>
</div>
</div>
</section>
<section id="structure">
<h3>Structure<a class="headerlink" href="#structure" title="Permalink to this heading"></a></h3>
<p>When writing CUDA C, it is convention to put the CUDA code into <code class="code docutils literal notranslate"><span class="pre">.cu</span></code> files and the CPU code in <code class="code docutils literal notranslate"><span class="pre">.c</span></code> files.
Even when using a library like cuSolver, it is still convention to place that code into <code class="code docutils literal notranslate"><span class="pre">.cu</span></code> files as we still
need to access some CUDA library functions, such as <code class="code docutils literal notranslate"><span class="pre">cudaMalloc</span></code> or <code class="code docutils literal notranslate"><span class="pre">cudaMemCpy</span></code>.</p>
<p>The CUDA code associated with matrix parallelisation has been written in the file <code class="code docutils literal notranslate"><span class="pre">$PYTHON/source/matrix_gpu.cu</span></code>
with the header file <code class="code docutils literal notranslate"><span class="pre">$PYTHON/source/matrix_gpu.h</span></code> which includes the function prototypes for the GPU matrix code.
The GSL matrix code is kept in <code class="code docutils literal notranslate"><span class="pre">$PYTHON/source/matrix_cpu.c</span></code> with function prototypes in
<code class="code docutils literal notranslate"><span class="pre">$PYTHON/source/templates.h</span></code>.</p>
<p>To be able to switch between the CUDA and GSL matrix implementations with the minimal amount of code changes, a
<code class="code docutils literal notranslate"><span class="pre">solve_matrix</span></code> wrapper function has been created. Either GSL or cuSolver is called within this wrapper, depending
on if Python was compiled with the flag <code class="code docutils literal notranslate"><span class="pre">-DCUDA_ON</span></code> as discussed earlier. This wrapper takes on the same name as
the original GSL implementation, meaning no code changes have occurred in that regard.</p>
<div class="literal-block-wrapper docutils container" id="id6">
<div class="code-block-caption"><span class="caption-text">The wrapper function which calls the appropriate matrix solver</span><a class="headerlink" href="#id6" title="Permalink to this code"></a></div>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;matrix_gpu.h&quot;</span><span class="c1">  /* The function prototype for gpu_solve_matrix is in here */</span>

<span class="kt">int</span>
<span class="nf">solve_matrix</span><span class="p">(</span><span class="kt">double</span><span class="w"> </span><span class="o">*</span><span class="n">a_matrix</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="o">*</span><span class="n">b_vector</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">matrix_size</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="o">*</span><span class="n">x_vector</span><span class="p">)</span>
<span class="p">{</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">error</span><span class="p">;</span>

<span class="cp">#ifdef CUDA_ON</span>
<span class="w">    </span><span class="n">error</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">gpu_solve_matrix</span><span class="p">(...);</span><span class="w">  </span><span class="cm">/* CUDA implementation */</span>
<span class="cp">#else</span>
<span class="w">    </span><span class="n">error</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cpu_solve_matrix</span><span class="p">(...);</span><span class="w">  </span><span class="cm">/* GSL implementation */</span>
<span class="cp">#endif</span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">error</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<p>The following code exert is an example of using the wrapper function to solve a linear system.</p>
<div class="literal-block-wrapper docutils container" id="id7">
<div class="code-block-caption"><span class="caption-text">The API to solve a linear system hasn’t changed</span><a class="headerlink" href="#id7" title="Permalink to this code"></a></div>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;python.h&quot;</span>

<span class="kt">double</span><span class="w"> </span><span class="o">*</span><span class="n">populations</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">malloc</span><span class="p">(</span><span class="n">nions</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="o">*</span><span class="n">populations</span><span class="p">));</span>
<span class="kt">double</span><span class="w"> </span><span class="o">*</span><span class="n">ion_density</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">malloc</span><span class="p">(</span><span class="n">nions</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="o">*</span><span class="n">populations</span><span class="p">));</span>
<span class="kt">double</span><span class="w"> </span><span class="o">*</span><span class="n">rate_matrix</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">malloc</span><span class="p">(</span><span class="n">nions</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">nions</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="o">*</span><span class="n">populations</span><span class="p">));</span>

<span class="n">populate_matrices</span><span class="p">(</span><span class="n">rate_matrix</span><span class="p">,</span><span class="w"> </span><span class="n">ion_density</span><span class="p">);</span>

<span class="cm">/* The wrapper function is named the same as the original GSL implementation</span>
<span class="cm">   and accepts the same arguments */</span>
<span class="kt">int</span><span class="w"> </span><span class="n">error</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">solve_matrix</span><span class="p">(</span>
<span class="w">    </span><span class="n">rate_matrix</span><span class="p">,</span><span class="w"> </span><span class="n">ion_density</span><span class="p">,</span><span class="w"> </span><span class="n">nions</span><span class="p">,</span><span class="w"> </span><span class="n">populations</span><span class="p">,</span><span class="w"> </span><span class="n">xplasma</span><span class="o">-&gt;</span><span class="n">nplasma</span>
<span class="p">);</span>

<span class="cm">/* One user difference is that error handling is more robust now, and there</span>
<span class="cm">   is a function to convert error codes into error messages */</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">error</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">EXIT_SUCCESS</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">Error</span><span class="p">(</span>
<span class="w">        </span><span class="s">&quot;Error whilst solving for ion populations: %d (%d)</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="n">get_matrix_error_string</span><span class="p">(</span><span class="n">error</span><span class="p">),</span><span class="w"> </span><span class="n">error</span>
<span class="w">    </span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<p>Here is an example of using a similar wrapper function to calculate the inverse of a matrix.</p>
<div class="literal-block-wrapper docutils container" id="id8">
<div class="code-block-caption"><span class="caption-text">The API has changed slightly for calculating the inverse, now that it has a wrapper function</span><a class="headerlink" href="#id8" title="Permalink to this code"></a></div>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;python.h&quot;</span>

<span class="kt">double</span><span class="w"> </span><span class="n">Q_matrix</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">malloc</span><span class="p">(</span><span class="n">matrix_size</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">matrix_size</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">double</span><span class="p">));</span>
<span class="kt">double</span><span class="w"> </span><span class="n">Q_inverse</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">malloc</span><span class="p">(</span><span class="n">matrix_size</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">matrix_size</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">double</span><span class="p">));</span>

<span class="n">populate_Q_matrix</span><span class="p">(</span><span class="n">Q_matrix</span><span class="p">);</span>

<span class="cm">/* The API is only different in the sense that a wrapper function now</span>
<span class="cm">   exists for matrix inversion */</span>
<span class="kt">int</span><span class="w"> </span><span class="n">error</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">invert_matrix</span><span class="p">(</span>
<span class="w">    </span><span class="n">Q_matrix</span><span class="p">,</span><span class="w"> </span><span class="n">Q_inverse</span><span class="p">,</span><span class="w"> </span><span class="n">matrix_size</span>
<span class="p">);</span>

<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">error</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">EXIT_SUCCESS</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">Error</span><span class="p">(</span>
<span class="w">        </span><span class="s">&quot;Error whilst solving for ion populations: %d (%d)</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="n">get_matrix_error_string</span><span class="p">(</span><span class="n">error</span><span class="p">),</span><span class="w"> </span><span class="n">error</span>
<span class="w">    </span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<p>To write the cuSolver implementation is similar to the GSL implementation, in that memory/resource are allocated for
cuSolver and then the appropriate library functions are called. The code exert below shows an illustrated (and
simplified) example of the cuSolver implementation to solve a linear system.</p>
<div class="literal-block-wrapper docutils container" id="id9">
<div class="code-block-caption"><span class="caption-text">An illustrative example of using cuSolver to solve a linear system using LU decomposition</span><a class="headerlink" href="#id9" title="Permalink to this code"></a></div>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;cuSolverDn.h&gt;</span>

<span class="k">extern</span><span class="w"> </span><span class="s">&quot;C&quot;</span><span class="w"> </span><span class="kt">int</span><span class="w">  </span><span class="cm">/* extern &quot;C&quot; has to be used to make it available to the C run time */</span>
<span class="n">gpu_solve_matrix</span><span class="p">(</span><span class="kt">double</span><span class="w"> </span><span class="o">*</span><span class="n">a_matrix</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="o">*</span><span class="n">b_vector</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">matrix_size</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="o">*</span><span class="n">x_vector</span><span class="p">)</span>
<span class="p">{</span>
<span class="w">    </span><span class="cm">/* First of all, allocate memory on the GPU and copy data from the CPU to the</span>
<span class="cm">       GPU. This uses the CUDA standard library functions, such as cudaMemCpy and</span>
<span class="cm">       cudaMalloc. This is part of the code is what takes the most time. */</span>
<span class="w">    </span><span class="n">allocate_memory_for_gpu</span><span class="p">();</span>
<span class="w">    </span><span class="n">copy_data_to_gpu</span><span class="p">();</span>

<span class="w">    </span><span class="cm">/* cuSolver and cuBLAS are both ports of Fortran libraries, which expect arrays to</span>
<span class="cm">    be in column-major format and we therefore need to transpose our row-major arrays */</span>
<span class="w">    </span><span class="n">transpose_row_to_column_major</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid_dim</span><span class="p">,</span><span class="w"> </span><span class="n">block_dim</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span>
<span class="w">        </span><span class="n">d_matrix_row</span><span class="p">,</span><span class="w"> </span><span class="n">d_matrix_col</span><span class="p">,</span><span class="w"> </span><span class="n">matrix_size</span>
<span class="w">    </span><span class="p">);</span>

<span class="w">    </span><span class="cm">/* Perform LU decomposition. Variables prefixed with d_ are kept in GPU memory where we</span>
<span class="cm">    allocated space for them in `allocate_memory_for_gpu` */</span>
<span class="w">    </span><span class="n">CUSOLVER_CHECK</span><span class="p">(</span><span class="n">cusolverDnDgetrf</span><span class="p">(</span>
<span class="w">        </span><span class="n">CUSOLVER_HANDLE</span><span class="p">,</span><span class="w"> </span><span class="n">matrix_size</span><span class="p">,</span><span class="w"> </span><span class="n">matrix_size</span><span class="p">,</span><span class="w"> </span><span class="n">d_matrix_col</span><span class="p">,</span><span class="w"> </span><span class="n">matrix_size</span><span class="p">,</span>
<span class="w">        </span><span class="n">d_workspace</span><span class="p">,</span><span class="w"> </span><span class="n">d_pivot</span><span class="p">,</span><span class="w"> </span><span class="n">d_info</span>
<span class="w">    </span><span class="p">));</span>

<span class="w">    </span><span class="cm">/* Solve the linear system A x = b. The final solution is returned in the</span>
<span class="cm">       variable d_v_vector */</span>
<span class="w">    </span><span class="n">CUSOLVER_CHECK</span><span class="p">(</span><span class="n">cusolverDnDgetrs</span><span class="p">(</span>
<span class="w">        </span><span class="n">CUSOLVER_HANDLE</span><span class="p">,</span><span class="w"> </span><span class="n">CUSOLVER_OP_N</span><span class="p">,</span><span class="w"> </span><span class="n">matrix_size</span><span class="p">,</span><span class="w"> </span><span class="n">matrix_size</span><span class="p">,</span><span class="w"> </span><span class="n">d_matrix_col</span><span class="p">,</span>
<span class="w">        </span><span class="n">matrix_size</span><span class="p">,</span><span class="w"> </span><span class="n">d_pivot</span><span class="p">,</span>
<span class="w">        </span><span class="n">d_b_vector</span><span class="p">,</span><span class="w"> </span><span class="n">matrix_size</span><span class="p">,</span><span class="w"> </span><span class="n">d_info</span>
<span class="w">    </span><span class="p">));</span>

<span class="w">    </span><span class="cm">/* We now have to copy d_b_vector back to the CPU, so we can use that value in</span>
<span class="cm">    the rest of Python */</span>
<span class="w">    </span><span class="n">copy_data_to_cpu</span><span class="p">();</span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">EXIT_SUCCESS</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<p>The naming conventions of cuSolver are discussed <a class="reference external" href="https://docs.nvidia.com/cuda/cusolver/index.html#naming-conventions">here</a>. In the case above, <code class="code docutils literal notranslate"><span class="pre">cuSolverDnDgetrf</span></code>
corresponds to: cusolverDn = <em>cuSolver Dense Matrix</em>, D = <em>double precision (double)</em> and getrf = <em>get right
hand factorisation</em>.</p>
<p>The most important thing to note, which may appear trivial, is the <code class="code docutils literal notranslate"><span class="pre">extern</span></code> keyword. Without this, when the
program is compiled the function <code class="code docutils literal notranslate"><span class="pre">gpu_solve_matrix</span></code> will not be available to the C runtime. By labelling the
function as <code class="code docutils literal notranslate"><span class="pre">extern</span> <span class="pre">&quot;C&quot;</span></code>, we make it clear that we want this function to be available to C source code. This only
needs to be done at the function definition, and not the function prototype in, e.g., a header file.</p>
</section>
<section id="compiling-and-linking">
<h3>Compiling and Linking<a class="headerlink" href="#compiling-and-linking" title="Permalink to this heading"></a></h3>
<p>CUDA code is compiled using the NVIDIA CUDA Compiler <code class="code docutils literal notranslate"><span class="pre">nvcc</span></code>. To combine both CPU and GPU code, the source must be
compiled with the respective compilers (e.g. <code class="code docutils literal notranslate"><span class="pre">gcc</span></code>/<code class="code docutils literal notranslate"><span class="pre">mpicc</span></code> for C and <code class="code docutils literal notranslate"><span class="pre">nvcc</span></code> for CUDA) to object code
(<code class="code docutils literal notranslate"><span class="pre">.o</span></code> files) and which are linked together using the C compiler with the appropriate library flags. In addition to
needing to link the cuSolver library (<code class="code docutils literal notranslate"><span class="pre">-lcusolver</span></code>) we also need to link the CUDA runtime library
(<code class="code docutils literal notranslate"><span class="pre">-lcudart</span></code>) when linking with the C compiler, which makes the standard CUDA library functions available to the C
compiler and runtime.</p>
<p>The steps for compiling and link GPU and CPU code are outlined below in pseudo-Makefile code.</p>
<div class="literal-block-wrapper docutils container" id="id10">
<div class="code-block-caption"><span class="caption-text">A brief overview on how to compile and link C and CUDA code</span><a class="headerlink" href="#id10" title="Permalink to this code"></a></div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define compilers for C and CUDA. When creating a CUDA/MPI application, we can</span>
<span class="c1"># just as easily use mpicc for our C compiler. It makes no difference.</span>
<span class="nv">CC</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>mpicc
<span class="nv">NVCC</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>nvcc

<span class="c1"># Define C and CUDA libraries. We still include GSL as other GSL numerical routines are</span>
<span class="c1"># used in Python</span>
<span class="nv">C_LIBS</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>-lgsl<span class="w"> </span>-lgslcblas<span class="w"> </span>-lm
<span class="nv">CUDA_LIBS</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>-lcudart<span class="w"> </span>-lcusolver

<span class="c1"># Define flags for C and CUDA compilers. -DCUDA_ON is used to conditionally compile</span>
<span class="c1"># to use the CUDA wrappers and other things related to the CUDA build</span>
<span class="nv">C_FLAGS</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>-O3<span class="w"> </span>-DCUDA_ON<span class="w"> </span>-DMPI_ON<span class="w"> </span>-I../includes<span class="w"> </span>-L../libs
<span class="nv">CUDA_FLAGS</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>-O3<span class="w"> </span>-DCUDA_ON

<span class="c1"># Compile CUDA source to object code using the CUDA compiler</span>
<span class="k">$(</span>NVCC<span class="k">)</span><span class="w"> </span><span class="k">$(</span>CUDA_FLAGS<span class="k">)</span><span class="w"> </span><span class="k">$(</span>CUDA_SOURCE<span class="k">)</span><span class="w"> </span>-c<span class="w"> </span>-o<span class="w"> </span><span class="k">$(</span>CUDA_OBJECTS<span class="k">)</span>

<span class="c1"># Compile the C code using the C compiler</span>
<span class="k">$(</span>CC<span class="k">)</span><span class="w"> </span><span class="k">$(</span>C_FLAGS<span class="k">)</span><span class="w"> </span><span class="k">$(</span>C_SOURCE<span class="k">)</span><span class="w"> </span>-c<span class="w"> </span>-o<span class="w"> </span><span class="k">$(</span>C_OBJECTS<span class="k">)</span>

<span class="c1"># Link the CUDA and C object code and libraries together using the C compiler</span>
<span class="k">$(</span>CC<span class="k">)</span><span class="w"> </span><span class="k">$(</span>CUDA_OBJECTS<span class="k">)</span><span class="w"> </span><span class="k">$(</span>C_OBJECTS<span class="k">)</span><span class="w"> </span>-o<span class="w"> </span>python<span class="w"> </span><span class="k">$(</span>CUDA_LIBS<span class="k">)</span><span class="w"> </span><span class="k">$(</span>C_LIBS<span class="k">)</span>
</pre></div>
</div>
</div>
<p>These steps are effectively replicated in the Makefile <code class="code docutils literal notranslate"><span class="pre">$PYTHON/source/Makefile</span></code>, where a deconstructed example is
shown below.</p>
<div class="literal-block-wrapper docutils container" id="id11">
<div class="code-block-caption"><span class="caption-text">The variables and recipes associated with CUDA are all conditional on NVCC being defined</span><a class="headerlink" href="#id11" title="Permalink to this code"></a></div>
<div class="highlight-Makefile notranslate"><div class="highlight"><pre><span></span><span class="c"># If NVCC has been set in the Makefile, then we can define CUDA_FLAG = -DCUDA_ON,</span>
<span class="c"># and the CUDA sources, which, at the moment, uses a wildcard to find all .cu files</span>
<span class="cp">ifneq ($(NVCC), )</span>
<span class="w">    </span><span class="nv">CUDA_FLAG</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>-DCUDA_ON
<span class="w">    </span><span class="nv">CUDA_SOURCE</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">$(</span>wildcard<span class="w"> </span>*.cu<span class="k">)</span>
<span class="cp">else</span>
<span class="w">    </span><span class="nv">CUDA_FLAG</span><span class="w"> </span><span class="o">=</span>
<span class="w">    </span><span class="nv">CUDA_SOURCE</span><span class="w"> </span><span class="o">=</span>
<span class="cp">endif</span>

<span class="c"># Then the recipe to create CUDA object code looks like this. If NVCC is blank,</span>
<span class="c"># nothing happens in the recipe</span>
<span class="nf">$(CUDA_OBJECTS)</span><span class="o">:</span><span class="w"> </span><span class="k">$(</span><span class="nv">CUDA_SOURCE</span><span class="k">)</span>
<span class="cp">ifneq ($(CUDA_FLAG),)</span>
<span class="w">    </span><span class="k">$(</span>NVCC<span class="k">)</span><span class="w"> </span><span class="k">$(</span>NVCC_FLAGS<span class="k">)</span><span class="w"> </span>-DCUDA_ON<span class="w"> </span>-I<span class="k">$(</span>INCLUDE<span class="k">)</span><span class="w"> </span>-c<span class="w"> </span>$&lt;<span class="w"> </span>-o<span class="w"> </span><span class="nv">$@</span>
<span class="cp">endif</span>

<span class="c"># So to compile Python, we have something which looks vaguely like this. Note that</span>
<span class="c"># we use the CUDA_OBJECTS recipe as a requirement for the python recipe. This CUSOLVER_STATUS_SUCCESS</span>
<span class="c"># the CUDA source to be compiled to object code *if* NVCC is defined</span>
<span class="nf">python</span><span class="o">:</span><span class="w"> </span><span class="n">startup</span> <span class="n">python</span>.<span class="n">o</span> <span class="k">$(</span><span class="nv">python_objects</span><span class="k">)</span> <span class="k">$(</span><span class="nv">CUDA_OBJECTS</span><span class="k">)</span>
<span class="w">    </span><span class="k">$(</span>CC<span class="k">)</span><span class="w"> </span><span class="k">$(</span>CFLAGS<span class="k">)</span><span class="w"> </span>python.o<span class="w"> </span><span class="k">$(</span>python_objects<span class="k">)</span><span class="w"> </span><span class="k">$(</span>CUDA_OBJECTS<span class="k">)</span><span class="w"> </span><span class="k">$(</span>kpar_objects<span class="k">)</span><span class="w"> </span><span class="k">$(</span>LDFLAGS<span class="k">)</span><span class="w"> </span>-o<span class="w"> </span>python
</pre></div>
</div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="programmer_notes.html" class="btn btn-neutral float-left" title="Programming Notes" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="tests.html" class="btn btn-neutral float-right" title="Unit Test Framework" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2018-2023, Knox Long, Christian Knigge, Stuart Sim, Nick Higginbottom, James Matthews, Sam Mangham, Edward Parkinson, Mandy Hewitt.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>